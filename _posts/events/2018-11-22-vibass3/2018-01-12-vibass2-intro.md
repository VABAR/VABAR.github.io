---
layout: page
title: "VIBASS2 - Basic Course"
subheadline: "Introduction to Bayesian Learning"
meta_teaser: "VIBASS2 Basic Course"
teaser: VIBASS2 Basic Course
header:
  title: VIBASS 2
  image_fullwidth: header_vibass18.png
  caption: Registration form
image:
  thumb: widget_vibass2.png
  homepage: vibass2.jpg
categories: events
---

The first two days include a basic course on Bayesian learning (12
hours), with conceptual sessions in the morning and practical sessions
with basic Bayesian packages in the afternoon. This is a summary of the
contents of both days.


## Monday 16


### Session I: Theory (10:00 -- 11:30)

__Introduction__. All you need is... probability. __Proportions__:
binomial distribution and likelihood function. __Prior distribution__: the
beta distribution. Posterior distribution is also a beta distribution.
__Summarising__ posterior inferences. __Estimation and prediction__. Prediction
of new binomial data. Inference and prediction with simulated samples:
comparison of independent populations.

### Session II: Theory (12:00 -- 13:30)

__Count data__: Poisson distribution. Poisson model parameterized in terms
of rate and exposure. Gamma distribution as __conjugate prior
distributions__. Negative binomial __predictive distributions__. __Normal data__. Estimation of a normal mean with known variance. __Prediction__ of a future
observation. Normal data with unknown mean and variance. __Nuisance
parameters__. __Joint prior distributions__. Joint, conditional and marginal
__posterior distributions__. __Hypothesis testing. Bayes factor__.

### Session III and IV: Practice (15:00 -- 16.30, 17:00 -- 18:30)

All you need is... lacasitos, Winterfell, and to measure your
height. Conceptual and computational issues for the Beta-Binomial,
Poisson-Gamma, and Normal-Normal models.


## Tuesday 17

### Session V: Theory (10:00 -- 11.30)

__Bayesian statistical modelling__. Starting with linear and generalized
linear models and understanding the basics of how to model a real
problem from the Bayesian point of view. Response variables, covariates,
factors (fixed and random).


### Session VI: Theory (12:00 -- 13.30)

__The big problem__ in the Bayesian framework: __resolution of integrals__ that
appear when applying the learning process. __Numerical approaches__: Laplace
approximations, __Monte Carlo integration__ and importance sampling. __Markov
Chain Monte Carlo__: Gibbs sampling and Metropolis Hastings. Convergence,
inspection of chains, etc. Examples of MCMC. Software for performing
MCMC. __Hierarchical Bayesian modeling__. Hierarchies or levels. Parameters
and hyperparameters. Priors and hyperpriors.


### Session VII and VIII: Practice (15:00 -- 16.30, 17:00-18:30)

Programming your own Metropolis-Hasting algorithm for the data and
models of the Sessions III and IV. `R` Software for inference in Bayesian
hierarchical models.
