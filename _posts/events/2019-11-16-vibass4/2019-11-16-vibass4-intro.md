---
layout: page
title: "VIBASS4 - Basic Course"
subheadline: "Introduction to Bayesian Learning"
meta_teaser: "VIBASS4 Basic Course"
teaser: VIBASS4 Basic Course
header:
  title: VIBASS 4
  image_fullwidth: header_vibass18.png
  caption: Registration form
image:
  thumb: widget_vibass4.png
  homepage: vibass4.jpg
categories: events
---

The first two days include a basic course on Bayesian learning (12
hours), with conceptual sessions in the morning and practical sessions
with basic Bayesian packages in the afternoon. This is a summary of the
contents of both days.

## Monday

### Session I: __Introduction to Bayesian statistics__ (10:00 -- 11:30)

All you need is... probability. Frequentist and Bayesian probability. Bayes’ theorem for random events and variables,
parameters, hypothesis, etc. Sequential updating. Predictive probabilities.
__Proportions__: binomial distribution and likelihood function. __Prior distribution__: the beta distribution. __Summarising__ posterior inferences. __Estimation and prediction__. Simulated samples: comparison of independent populations.

### Session I: Practice (12:00 -- 12:30)

All you need is... lacasitos.


### Session II: __Basic statistical models__ (15:00 -- 16:30)

__Count data__: Poisson distribution. Poisson model parameterized in terms
of rate and exposure. Gamma distribution as __conjugate prior
distributions__. Negative binomial __predictive distributions__. 

__Normal data__: Estimation of a normal mean with known variance. __Prediction__ of a future observation. Normal data with unknown mean and variance. __Nuisance
parameters__. __Joint prior distributions__. Joint, conditional and marginal
__posterior distributions__. 


### Session II: Practice (17:00 -- 18:30)

How many u’s in a _Game of Thrones_ book page and how tall are you?


## Tuesday

### Session III: __Bayesian inference__ (10:00 -- 11:30)

The big problem in the Bayesian framework: resolution of integrals that appear when applying the learning process.
__Numerical approaches__: Gaussian approximations, Laplace approximations, Monte Carlo integration and importance sampling. __Markov chain Monte Carlo__: Gibbs sampling and Metropolis Hastings. Convergence, inspection of chains,
etc. __Software__ for performing MCMC.


### Session III: Practice (12:00 -- 13:30)

Programming your own Metropolis-Hasting algorithm.


### Session IV: __Bayesian hierarchical models__ (15:00 -- 16:30)

Incorporating _random_ effects: __Bayesian hierarchical models__ (BHMs), the coolest tool for modelling highly structured models. Hierarchies, hyperparameters, and hyperpriors. (Generalized) linear mixed models as basic examples of BHMs.

### Session IV: Practice (17:00 -- 18:30)

__Software__ for inference in Bayesian hierarchical models.
